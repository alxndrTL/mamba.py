{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the PyTorch parallel scan, with RNNs/Mamba in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file gives a detailed background and explanation of the `pscan.py` file. The goal is to impement a parallel scan in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, what is a <b>scan</b> ?\n",
    "\n",
    "A scan is defined as an operation that takes as input an array and procudes an array as output. You can see that it is quite general.\n",
    "\n",
    " A simple and well-known example of a scan is the <i>cumulative sum of an array</i> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  6, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "torch.cumsum(X, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this document, we will denote `L` as the length of our input array `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic way to implement a scan is to use a simple for loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  6, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.zeros_like(X)\n",
    "\n",
    "cumulative_sum = 0\n",
    "for t in range(X.size(0)):\n",
    "    cumulative_sum += X[t]\n",
    "    Y[t] = cumulative_sum\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite simple for now, right ?\n",
    "To setup our notations, we will keep this example for a bit.\n",
    "\n",
    "Here, we use an accumulator, `cumulative_sum`, which we update as we go through the input array `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent way to rewrite the above code is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  6, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.zeros_like(X)\n",
    "\n",
    "Y[0] = X[0]\n",
    "for t in range(1, X.size(0)):\n",
    "    Y[t] = Y[t-1] + X[t]\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not explicitly present, the accumulator is still here, in `Y`. It is propagated with the recurrence relation `Y[t] = Y[t-1] + X[t]`.\n",
    "\n",
    "We can visualize what happens with a simple diagram, which should remind you a bit about RNNs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"assets/cumsum_rnns.jpg\" alt=\"cumulative sum\" width=\"1000\" height=\"300\" alt=\"python mamba\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some sense, `Y` plays the role of the hidden state, while `X` plays the role of the input : as we process the input, we keep and update a running hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this method of computation, which uses a sequential loop, induces `L` sequential steps of computations in order to compute the whole output `Y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, is it possible to <b>parallelize</b> this scan operation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just what does the <b>parallel scan</b>. \n",
    "\n",
    "Let's stay with the simple example of our cumulative sum. In fact, let's simplify it even more : let's say with just want to compute the sum of our input array `X`.\n",
    "Again, we could come up with a for loop to count the elements. But can't we parallelize this computation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, and it is best visualized with this simple tree :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"assets/reduction_tree.jpg\" alt=\"cumulative sum\" width=\"1000\" height=\"600\" alt=\"python mamba\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the length of our array is a power of 2, then we just have to group the elements 2 by 2, add them, and repeat until we are left with one element, our result. If `L` is `2**d`, then we will need to do `math.log2(L)` sequential steps to compute the sum of the array. That's a major speedup over the `L` steps of the naive for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we implement this in Python ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]) # input array\n",
    "L = X.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can <i>group the elements by two</i> using :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa = X.view(L//2, 2)\n",
    "Xa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have pairs of elements. We can do :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 8])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to access the elements from the two groups. We can see that, to compute the first step, we simply need to sum these two arrays :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  7, 11, 15])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 0] + Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay ! We have just accomplished our first step ! Now, we just need to repeat what we've just done.\n",
    "\n",
    "Note that we could do `Xa = Xa[:, 0] + Xa[:, 1]` and then we would just need to repeat the previous step with `Xa`. But :\n",
    "- this will allocate extra memory spaces for storing the result of the first step (`Xa` currently shares the data as `X`, doing this would compute the sum and store its output in a new memory space).\n",
    "- we will reuse some of these values, later, for the full scan operation.\n",
    "\n",
    "Hence, we will work by updating `X` <b>in-place</b>, by doing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  7, 11, 15])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 1] += Xa[:, 0]\n",
    "Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we want. Now, we will repeat the step we have just done, but on `Xa[:, 1]` rather than on `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa = Xa[:, 1]\n",
    "Xa = Xa.view(Xa.size(0)//2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, `Xa` is split in two groups :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  7],\n",
       "        [11, 15]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sum these two groups, and put the result in the second half of `Xa`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 26])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 1] += Xa[:, 0]\n",
    "Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two elements left ! This means, only one more step to go (because `math.log2(2) = 1` of course !) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa = Xa[:, 1]\n",
    "Xa = Xa.view(Xa.size(0)//2, 2)\n",
    "\n",
    "Xa[:, 1] += Xa[:, 0]\n",
    "Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the result we want !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, here is the full code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  3, 10,  5, 11,  7, 36])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]) # input array\n",
    "L = X.size(0)\n",
    "\n",
    "Xa = X\n",
    "\n",
    "for k in range(int(math.log2(L))):\n",
    "    T = 2 * (Xa.size(0) // 2)\n",
    "\n",
    "    # split into 2 groups of pairs of elements\n",
    "    Xa = Xa.view(T//2, 2) \n",
    "\n",
    "    # for each pair, add the first to the second\n",
    "    Xa[:, 1].add_(Xa[:, 0])\n",
    "\n",
    "    # change the view for the next iteration\n",
    "    Xa = Xa[:, 1]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our result is found in the last element of `X` (remember that all the `Xa`'s are views of our original tensor!)\n",
    "\n",
    "Let's look at how `X` evolved during this whole process : each line corresponds to the data of tensor `X` at a particular iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# schema tensor evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help clarify, we can also draw on top of this the tree underlying the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# schema tensor evolution with tree drawn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that having all these numbers is a bit cumbersome, as only a few of them changes at each iteration.\n",
    "\n",
    "So, for the rest of this document, we'll represent this with the following diagram :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# schema tree reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is essentially a simple tree again, **but** :\n",
    "- the placement of the nodes is important : see how the sum of each pair is placed on the element of the right.\n",
    "- at each level of the tree, we can easily see the exact state of the array : you just have, for each position, to look at the upmost element in the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we can compute the sum of an array in parallel. Now, what about the cumulative sum of an array ?\n",
    "\n",
    "If we look again at the previous tree, we can see that the values of some nodes are partial sums of our input array. For example, the 10 is the sum of elements 1 to 4.\n",
    "\n",
    "In fact, it's quite easy to see (and demonstrate) that each node holds the sum of the elements of `X` which are its children or more generally its descendants.\n",
    "\n",
    "We can see it even more clearly here :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#schema tree reduction 0->7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consedering this, it's now becoming clearer that we will be able to reuse this tree to construct the cumulative sum of `X`. That's why we used in-place operations in the code above : after the process of computing the sum, we have left in `X` some values of this tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
