{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciej/miniconda3/envs/llm-random/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from mamba_lm import from_pretrained\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciej/miniconda3/envs/llm-random/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = from_pretrained('state-spaces/mamba-130m').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.forward(\n",
    "    torch.tensor([tokenizer.encode(\"Hello, world!\"), tokenizer.encode(\"Hello, world!\")]))\n",
    "#.unsqueeze(0).to(device)\n",
    "# print(x)\n",
    "\n",
    "x = model.forward_with_caches(\n",
    "    torch.tensor([tokenizer.encode(\"Hello, world!\"), tokenizer.encode(\"Hello, world!\")]), requested_caches=[1, 1])\n",
    "# .unsqueeze(0).to(device)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "with torch.no_grad():\n",
    "   model.eval()\n",
    "   bs, seqlen = 10, 10\n",
    "   split = 5\n",
    "\n",
    "   requested_caches = [split] * bs\n",
    "   input = torch.randint(0, 50256, (bs, seqlen)).to(device)\n",
    "   x, our_caches = model.forward_with_caches(input, requested_caches=requested_caches)\n",
    "   # for i in range(split+1, bs):\n",
    "   #    logits, our_caches = model.step(input[:, i-1], caches=copy.deepcopy(our_caches))\n",
    "   # final_logits = logits.clone()\n",
    "\n",
    "   # reference_logits = model(input)\n",
    "   # reference_caches = [(None, torch.zeros(bs, model.config.d_inner, model.config.d_conv-1, device='cpu')) for _ in range(model.config.n_layers)]\n",
    "   reference_caches = copy.deepcopy(our_caches)\n",
    "   for i in range(split, seqlen):\n",
    "      logits_sequential, reference_caches = model.step(input[:, i], caches=reference_caches)\n",
    "\n",
    "   # for i in range(model.config.n_layers):\n",
    "   #    assert (our_caches[i][0] - reference_caches[i][0]).norm() < 1e-3\n",
    "   #    assert (our_caches[i][1] - reference_caches[i][1]).norm() < 1e-3\n",
    "\n",
    "\n",
    "   logits_parallel = model(input)\n",
    "\n",
    "\n",
    "   # print(torch.allclose(final_logits, reference_logits[:, split]))\n",
    "\n",
    "   # input = torch.tensor([tokenizer.encode(\"Hello, world!\"), tokenizer.encode(\"Hello, world!\")])\n",
    "   # x, our_caches = model.forward_with_caches(\n",
    "   #    input[:, 0:1], requested_caches=[1, 1])\n",
    "   # # our_caches = copy.deepcopy(our_caches)\n",
    "\n",
    "   # zeros = [(None, torch.zeros(2, model.config.d_inner, model.config.d_conv-1, device='cpu')) for _ in range(model.config.n_layers)]\n",
    "   # _, reference_caches = model.step(\n",
    "   #     torch.tensor([tokenizer.encode(\"Hello, world!\"), tokenizer.encode(\"Hello, world!\")])[:, 0], caches=zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0006)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(logits_parallel[:, seqlen-1] - logits_sequential).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.6566e-10, -9.3132e-10,  0.0000e+00,  ...,  3.7253e-09,\n",
       "          -7.4506e-09,  1.8626e-09],\n",
       "         [ 1.8626e-09, -2.9104e-11, -4.6566e-10,  ..., -2.3283e-10,\n",
       "           2.3283e-10, -5.5879e-09],\n",
       "         [ 2.3283e-10,  0.0000e+00, -2.9802e-08,  ..., -1.8626e-09,\n",
       "           0.0000e+00, -9.3132e-10],\n",
       "         ...,\n",
       "         [ 1.8626e-09,  1.8626e-09, -1.4901e-08,  ...,  0.0000e+00,\n",
       "          -2.2352e-08,  7.4506e-09],\n",
       "         [ 1.4901e-08, -2.9802e-08, -2.3283e-10,  ..., -5.5879e-09,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 4.6566e-10,  0.0000e+00, -3.7253e-09,  ...,  0.0000e+00,\n",
       "           0.0000e+00, -6.9849e-10]],\n",
       "\n",
       "        [[ 0.0000e+00, -2.3283e-10,  2.9802e-08,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-4.6566e-10, -2.9104e-11,  0.0000e+00,  ..., -1.8626e-09,\n",
       "           0.0000e+00,  9.3132e-10],\n",
       "         [ 9.3132e-10,  3.7253e-09, -1.4901e-08,  ..., -3.7253e-09,\n",
       "           1.4901e-08, -1.8626e-09],\n",
       "         ...,\n",
       "         [-3.7253e-09,  2.9104e-10,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          -2.9802e-08,  5.5879e-09],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -5.5879e-09,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 5.8208e-10,  2.9104e-11, -1.8626e-09,  ...,  1.2806e-09,\n",
       "          -9.3132e-10, -1.3970e-09]],\n",
       "\n",
       "        [[ 0.0000e+00,  2.3283e-10,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           1.8626e-09,  0.0000e+00],\n",
       "         [ 1.8626e-09, -2.9104e-10, -9.3132e-09,  ..., -1.1176e-08,\n",
       "           4.4703e-08, -7.4506e-09],\n",
       "         [-4.6566e-10, -1.4901e-08, -8.9407e-08,  ...,  2.7940e-09,\n",
       "          -7.4506e-09,  2.7940e-09],\n",
       "         ...,\n",
       "         [ 3.7253e-09, -1.1642e-10, -1.4901e-08,  ..., -7.4506e-09,\n",
       "          -7.4506e-09,  1.8626e-09],\n",
       "         [-3.7253e-09,  3.7253e-09, -4.6566e-10,  ...,  4.6566e-10,\n",
       "           2.7940e-09, -6.9849e-10],\n",
       "         [-4.6566e-10,  0.0000e+00,  0.0000e+00,  ...,  9.3132e-10,\n",
       "          -1.8626e-09,  0.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  1.4901e-08,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00, -2.3283e-10,  9.3132e-10,  ...,  3.7253e-09,\n",
       "          -3.7253e-09,  0.0000e+00],\n",
       "         [ 0.0000e+00,  3.7253e-09,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  3.7253e-09,  8.9407e-08,  ...,  7.4506e-09,\n",
       "           4.4703e-08, -5.5879e-09],\n",
       "         [ 7.4506e-09, -1.4901e-08, -1.1642e-10,  ..., -3.7253e-09,\n",
       "           1.8626e-09, -4.6566e-10],\n",
       "         [ 0.0000e+00,  2.3283e-10,  0.0000e+00,  ..., -9.3132e-10,\n",
       "           3.7253e-09, -9.3132e-10]],\n",
       "\n",
       "        [[ 0.0000e+00,  2.3283e-10,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 3.7253e-09,  1.1642e-10,  0.0000e+00,  ..., -7.4506e-09,\n",
       "           0.0000e+00, -3.7253e-09],\n",
       "         [-6.9849e-10, -7.4506e-09, -2.9802e-08,  ...,  3.2596e-09,\n",
       "          -7.4506e-09,  1.3970e-09],\n",
       "         ...,\n",
       "         [ 1.8626e-09,  2.6193e-10,  1.4901e-08,  ...,  7.4506e-09,\n",
       "           1.4901e-08,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00, -8.7311e-11,  ..., -3.7253e-09,\n",
       "          -6.9849e-10,  1.1642e-10],\n",
       "         [-1.8626e-09, -4.9477e-10,  5.1223e-09,  ...,  5.5879e-09,\n",
       "          -3.7253e-08,  1.8626e-09]],\n",
       "\n",
       "        [[ 9.3132e-10,  4.6566e-10,  1.4901e-08,  ..., -1.8626e-09,\n",
       "           7.4506e-09, -9.3132e-10],\n",
       "         [-4.6566e-10,  0.0000e+00, -6.9849e-10,  ..., -1.8626e-09,\n",
       "           7.4506e-09,  9.3132e-10],\n",
       "         [ 1.1642e-10,  3.7253e-09, -2.9802e-08,  ..., -4.6566e-10,\n",
       "           1.8626e-09, -2.3283e-10],\n",
       "         ...,\n",
       "         [-1.8626e-09,  0.0000e+00,  2.9802e-08,  ...,  1.4901e-08,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-1.8626e-08,  1.8626e-08, -1.1642e-10,  ...,  5.5879e-09,\n",
       "           9.3132e-10, -1.7462e-10],\n",
       "         [ 3.2596e-09, -6.9849e-10, -6.5193e-09,  ..., -5.5879e-09,\n",
       "           3.7253e-09, -7.4506e-09]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference_logits[:, -1:, :].shape\n",
    "# final_logits.shape\n",
    "our_caches[0][0] - reference_caches[0][0]\n",
    "# reference_logits - final_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4919e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0., grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(9.2723e-07, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.0019e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(4.7374e-07, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.2792e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.5112e-07, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.8428e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(1.3595e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.8514e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(1.0204e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.1822e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(1.6732e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.1905e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.0882e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.0574e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(1.7318e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.0373e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(1.8942e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.3323e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.6604e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.1054e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.0082e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.1930e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(1.5851e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.0161e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(4.2642e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.8179e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.2837e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.2022e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.4837e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.5063e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.5608e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.7580e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(1.8490e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.6979e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(5.9052e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.5207e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(5.8311e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(5.1071e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.5880e-06, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(5.9208e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.8527e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(4.4003e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(2.7611e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(4.2833e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(7.0859e-05, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(3.8067e-05, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for layer in range(model.config.n_layers):\n",
    "    for i in range(2):\n",
    "        print((our_caches[layer][i] - reference_caches[layer][i]).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, world!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, world!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/IDEAS/CL in ICL/speeding-up-mamba/mamba.py/mamba_lm.py:145\u001b[0m, in \u001b[0;36mMambaLM.step\u001b[0;34m(self, token, caches)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, token, caches):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# token : (B)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# caches : [cache(layer) for all layers], cache : (h, inputs)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# logits : (B, vocab_size)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# caches : [cache(layer) for all layers], cache : (h, inputs)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(token)\n\u001b[0;32m--> 145\u001b[0m     x, caches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmamba\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_f(x)\n\u001b[1;32m    148\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/Documents/IDEAS/CL in ICL/speeding-up-mamba/mamba.py/mamba.py:103\u001b[0m, in \u001b[0;36mMamba.step\u001b[0;34m(self, x, caches)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, caches):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# x : (B, L, D)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# caches : [cache(layer) for all layers], cache : (h, inputs)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# y : (B, L, D)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# caches : [cache(layer) for all layers], cache : (h, inputs)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 103\u001b[0m         x, caches[i] \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, caches\n",
      "File \u001b[0;32m~/Documents/IDEAS/CL in ICL/speeding-up-mamba/mamba.py/mamba.py:143\u001b[0m, in \u001b[0;36mResidualBlock.step\u001b[0;34m(self, x, cache)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cache):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# x : (B, D)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# cache : (h, inputs)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# output : (B, D)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# cache : (h, inputs)\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     output, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output, cache\n",
      "File \u001b[0;32m~/Documents/IDEAS/CL in ICL/speeding-up-mamba/mamba.py/mamba.py:474\u001b[0m, in \u001b[0;36mMambaBlock.step\u001b[0;34m(self, x, cache)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cache):\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# x : (B, D)\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# cache : (h, inputs)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# y : (B, D)\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# cache : (h, inputs)\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     h, inputs \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m    476\u001b[0m     xz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj(x) \u001b[38;5;66;03m# (B, 2*ED)\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     x, z \u001b[38;5;241m=\u001b[39m xz\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, ED), (B, ED)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "x = model.step(\n",
    "    torch.tensor([tokenizer.encode(\"Hello, world!\"), tokenizer.encode(\"Hello, world!\")]), caches=[None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
